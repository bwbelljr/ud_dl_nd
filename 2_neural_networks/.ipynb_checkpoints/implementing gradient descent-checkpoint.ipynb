{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing with NumPy\n",
    "\n",
    "First, you'll need to initialize the weights. We want these to be small such that the input to the sigmoid is in the linear region near 0 and not squashed at the high and low ends. It is also important to initialize them randomly so that they all have different starting values and diverge, breaking symmetry. We will initialize the weights from a normal distribution centered at 0. A good value for the scale is $1/\\sqrt{n}$ where $n$ is the number of input units. This keeps the input to the sigmoid low for increasing numbers of input units.\n",
    "\n",
    "```\n",
    "weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
    "```\n",
    "\n",
    "NumPy provides a function `np.dot()` that calculates the dot product of two arrays, which conveniently calculates $h$ for us. The dot product multiplies two arrays element-wise, the first element in array 1 is multiplied by the first element in array 2, and so on. Then each product is summed.\n",
    "\n",
    "```\n",
    "# input to the output layer\n",
    "output_in = np.dot(weights, inputs)\n",
    "```\n",
    "\n",
    "And finally, we can update $\\Delta w_i$ and $w_i$ by incrementing them with `weights += ...` which is shorthand for `weights = weights + ...`.\n",
    "\n",
    "## Efficiency tip!\n",
    "You can save some calculations since we're using a sigmoid here. For the sigmoid function, $f'(h) = f(h)(1-f(h))$. This means that once you calculate $f(h)$, the activation of the output unit, you can use it to calculate the gradient for the error gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming exercise\n",
    "\n",
    "Below, you will implement gradient descent and train the network on the admissions data. Your goal here is to train the network until you reach a minimum in the mean square error (MSE) on the training set. You need to implement:\n",
    "* The network output: `output`\n",
    "* The output error: `error`\n",
    "* The error term: `error_term`\n",
    "* Update the weight step: `del_w +=`\n",
    "* Update the weights: `weights +=`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_url = \"https://stats.idre.ucla.edu/stat/data/binary.csv\"\n",
    "admissions = pd.read_csv(csv_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:, field] = (data[field]-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.loc[sample, :], data.drop(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1/n_features**.5, size=n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2627609384996635\n",
      "Train loss: 0.20928619409324875\n",
      "Train loss: 0.20084292908073426\n",
      "Train loss: 0.19862156475527873\n",
      "Train loss: 0.19779851396686027\n",
      "Train loss: 0.19742577912189863\n",
      "Train loss: 0.1972350774624106\n",
      "Train loss: 0.1971294562509248\n",
      "Train loss: 0.19706766341315082\n",
      "Train loss: 0.19703005801777368\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "        \n",
    "        # Note: we have not included the h variable from the previous\n",
    "        #       lesson. You can add it if you want, or you can calculate\n",
    "        #       the h together with the output.\n",
    "        \n",
    "        # TODO: Calculate the output\n",
    "        output = sigmoid(np.dot(weights, x))\n",
    "        \n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "        \n",
    "        # Calculate the error term\n",
    "        # Observe that f prime = output * (1-output)\n",
    "        error_term = error*output*(1-output)\n",
    "        \n",
    "        # TODO: Calculate the change in weights for this sample\n",
    "        #       and add it to the total weight change\n",
    "        del_w += error_term*x\n",
    "    \n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += learnrate*del_w/n_records\n",
    "    \n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out-targets)**2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(f\"Train loss: {loss} WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(f\"Train loss: {loss}\")\n",
    "        last_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ud_dl_nd] *",
   "language": "python",
   "name": "conda-env-ud_dl_nd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
