{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"part_3_training_neural_networks.ipynb","provenance":[],"authorship_tag":"ABX9TyNW3sIDCMVGqUtfP9d4L16m"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"S5-szztTDdGh"},"source":["# Training Neural Networks\n","\n","The network we built in the previous part isn't so smart, and it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural network is that we can train them to approximate this function, and basically any function given enough data and compute time.\n","\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/raw/master/intro-to-pytorch/assets/function_approx.png\">\n","\n","At first the network is naive, and it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n","\n","To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems.\n","\n","$$\n","\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n","$$\n","\n","where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n","\n","\n","By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called *gradient descent*. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n","\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/raw/master/intro-to-pytorch/assets/gradient_descent.png\" width=350px>\n","\n","## Backpropagation\n","\n","For single layer network, gradient descent is straightforward to implement. However, it is more complicated for deeper, multilayer neural networks like the one we have built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks.\n","\n","Training multilayer network is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n","\n","<img src='https://github.com/udacity/deep-learning-v2-pytorch/raw/master/intro-to-pytorch/assets/backprop_diagram.png' width=550px>\n","\n","In the forward pass through the network, our data and operations go from bottom to top here. We pass the input $x$ through a linear transformation $L_1$ with weights $W_1$ and biases $b_1$. The output then goes through the sigmoid operation $S$ and another linear transformation $L_2$. Finally we calculate the loss $\\ell$. We use the loss as a measure of how bad the network's predictions are. The goal is to adjust the weights and biases to minimize the loss.\n","\n","To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs."]},{"cell_type":"code","metadata":{"id":"9yXEFJZUEElB"},"source":["|"],"execution_count":null,"outputs":[]}]}