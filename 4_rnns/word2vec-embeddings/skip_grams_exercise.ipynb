{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"skip_grams_exercise.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPnu4jLBD437QzfD8xLCMJf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"L267HcxuYbLX"},"source":["# Skip-gram Word2Vec\r\n","\r\n","In this notebook, I'll lead you through using PyTorch to implement the [Word2Vec algorithm](https://en.wikipedia.org/wiki/Word2vec) using the skip-gram architecture. By implementing this, you'll learn about embedding words for use in NLP. This will come in handy when dealinw with things like machine translation.\r\n","\r\n","## Readings\r\n","\r\n","Here are the resources used to build this notebook\r\n","* A [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of Word2Vec from Chris McCormick\r\n","* [First Word2Vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.\r\n","* [NIPS paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with improvements for Word2Vec also from Mikolov et al.\r\n","\r\n","---\r\n","\r\n","## Word Embeddings\r\n","\r\n","When you're dealing with words in text, you end up with tens of thousands of word classes to analyze; one for each word in a vocabulary. Trying to one-hot encode these words is massively inefficient because most values in a one-hot vector will be set to zero. So the matrix multiplication that happens in between a one-hot input vector and a first, hidden layer will result in mostly zero-valued hidden outputs.\r\n","\r\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/raw/3a95d118f9df5a86826e1791c5c100817f0fd924/word2vec-embeddings/assets/one_hot_encoding.png\">\r\n","\r\n","To solve this problem and greatly increase the efficiency of our network, we use what are called **embeddings**. Embeddings are just a fully connected layer like you've seen before. We call this layer the embedding layer and the weights are the embedding weights. We skip the multiplication into the embedding layer by instead directly grabbing the hidden layer values from the weight matrix. We can do this because the multiplication of a one-hot encoded vector with a matrix returns the row of the matrix corresponding to the index of the \"on\" input unit.\r\n","\r\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/raw/3a95d118f9df5a86826e1791c5c100817f0fd924/word2vec-embeddings/assets/lookup_matrix.png\">\r\n","\r\n","Instead of doing the matrix multplication, we use the weight matrix as a lookup table. We encode the words as integers, for example, \"heart\" is encoded as 958, \"mind\" as 18094. Then to get hidden layer values for \"heart\", you just take the 958th row of the embedding matrix. This process is called an **embedding lookup** and the number of hidden units is the **embedding dimension**.\r\n","\r\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/raw/3a95d118f9df5a86826e1791c5c100817f0fd924/word2vec-embeddings/assets/tokenize_lookup.png\">\r\n","\r\n","There is nothing magical going on here. The embedding lookup table is just a weight matrix. THe embedding layer is just a hidden layer. The lookup is just a shortcut for the matrix multiplication. The lookup table is trained just like any weight matrix.\r\n","\r\n","Embeddings aren't only used for words of course. You can use them for any model where you have a massive number of classes. A particular type of model called **Word2Vec** uses the embedding layer to find vector representations of words that contain semantic meaning.\r\n","\r\n","---\r\n","\r\n","## Word2Vec\r\n","\r\n","The Word2Vec algorithm finds much more efficient representations by finding vectors that represent the words. These vectors also contain semantic information about the words\r\n","\r\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/raw/3a95d118f9df5a86826e1791c5c100817f0fd924/word2vec-embeddings/assets/context_drink.png\">\r\n","\r\n","Words that show up in similar **contexts**, such as \"coffee\", \"tea\", and \"water\" will have vectors near each other. Different words will be further away from one another, and relationships can be represented by distance in vector space.\r\n","\r\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/raw/3a95d118f9df5a86826e1791c5c100817f0fd924/word2vec-embeddings/assets/vector_distance.png\">\r\n","\r\n","There are two architectures for implementing Word2Vec:\r\n","\r\n",">* CBOW (Continuous Bag-Of_words) and \r\n",">* Skip-gram\r\n","\r\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/raw/3a95d118f9df5a86826e1791c5c100817f0fd924/word2vec-embeddings/assets/word2vec_architectures.png\">\r\n","\r\n","In this implementation, we'll be using the **skip-gram architecture** because it performs better than CBOW. Here, we pass in a word and try to predict the words surrounding it in the text. In this way, we can train the network to learn representations for words that show up in similar contexts\r\n","\r\n","---\r\n","\r\n","## Loading Data\r\n","\r\n","We will load in data and place it in the `data` directory.\r\n","\r\n","1. Load the [text8 dataset](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip); a file of cleaned up *Wikipedia article text* from Matt Mahoney\r\n","2. Place that data in the `data` home directory\r\n","3. Then you can extract it and delete the archive, zip file to save storage space\r\n","\r\n","After following these steps, you should have one file in your data directory: `data/text8`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5CG0S3K1cpUj","executionInfo":{"status":"ok","timestamp":1611442407658,"user_tz":-60,"elapsed":2708,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"3c79639f-0d83-4ea1-8480-e74c39d99d74"},"source":["!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip\r\n","!mkdir ./data \r\n","!unzip text8.zip -d ./data"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2021-01-23 22:53:27--  https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.251.166\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.251.166|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 31344016 (30M) [application/zip]\n","Saving to: ‘text8.zip’\n","\n","text8.zip           100%[===================>]  29.89M  29.4MB/s    in 1.0s    \n","\n","2021-01-23 22:53:28 (29.4 MB/s) - ‘text8.zip’ saved [31344016/31344016]\n","\n","Archive:  text8.zip\n","caution: filename not matched:  ./data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rdMn3jAkc5ow","executionInfo":{"status":"ok","timestamp":1611442525726,"user_tz":-60,"elapsed":654,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"69afba7b-bc52-4e22-bc67-c67dcb40e351"},"source":["# read in the extracted text file\r\n","with open('data/text8') as f:\r\n","    text = f.read()\r\n","\r\n","# print out the first 100 characters\r\n","print(text[:100])"],"execution_count":4,"outputs":[{"output_type":"stream","text":[" anarchism originated as a term of abuse first used against early working class radicals including t\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gkJhHjNXdL8Q"},"source":["## Pre-processing"]},{"cell_type":"code","metadata":{"id":"cqp4XFNRdc0i"},"source":[""],"execution_count":null,"outputs":[]}]}