{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_gan_exercise.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP96x86wOIgZCqEYZTo8Djj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2a26dde2ea1d47eab49d7fc5ff27fef1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ef8f76ccba41452296b77e9d83253afb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_50486b253e09492a958ffca7263170db","IPY_MODEL_76c865cad5aa4509963501dd84643158"]}},"ef8f76ccba41452296b77e9d83253afb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"50486b253e09492a958ffca7263170db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dbf3c5b4c8c74704948f48628d5c46a0","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c9b5bc5865644115a5cc6bb41adb2862"}},"76c865cad5aa4509963501dd84643158":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b34dd037153e4740831dec7f89fe8647","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9920512/? [00:07&lt;00:00, 1337288.11it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4b804211c5c34f0090ca5163a715c006"}},"dbf3c5b4c8c74704948f48628d5c46a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c9b5bc5865644115a5cc6bb41adb2862":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b34dd037153e4740831dec7f89fe8647":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4b804211c5c34f0090ca5163a715c006":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b95899b999e48229afd0a5ab8e9d8f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fd349784bc3f4e8280c9d52b7cf3a0a1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_42ab7588bb9341888497c2e7ff704f97","IPY_MODEL_6ef0359d3f904df5b66d4cfe76393d18"]}},"fd349784bc3f4e8280c9d52b7cf3a0a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42ab7588bb9341888497c2e7ff704f97":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0550694132784348a12bc0bc5abcbb4d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f7749db8f1a645bfbdde8baf8a46dd4a"}},"6ef0359d3f904df5b66d4cfe76393d18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f33ea0f49abf4e5c89d3d2be2b9ed14f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 32768/? [00:00&lt;00:00, 46770.29it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5979c18084d5432095fb957398a5b4a4"}},"0550694132784348a12bc0bc5abcbb4d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f7749db8f1a645bfbdde8baf8a46dd4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f33ea0f49abf4e5c89d3d2be2b9ed14f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5979c18084d5432095fb957398a5b4a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dfcee6a6c834462c9a7fde243f9c1925":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_020672a5870b4aacb7100ded9aea84ca","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a130f54bf89244a78f2535ad52cba3d0","IPY_MODEL_7a736623a40d4534a43f5bb4fcae072f"]}},"020672a5870b4aacb7100ded9aea84ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a130f54bf89244a78f2535ad52cba3d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_db5dec59fe10404d9d255c8414aba6fe","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f4821c2fa7374e3983565f4293eba544"}},"7a736623a40d4534a43f5bb4fcae072f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_986c21b5e2c64b7792cee4b317e46d4c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1654784/? [00:03&lt;00:00, 435518.20it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_49637f35116e4d91af5b9f5a47a7ac46"}},"db5dec59fe10404d9d255c8414aba6fe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f4821c2fa7374e3983565f4293eba544":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"986c21b5e2c64b7792cee4b317e46d4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"49637f35116e4d91af5b9f5a47a7ac46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e64a8412e5f14f9bb79cb1705c3e7bc0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c8f6b37ce3e64f58a66cfc15a6ce39f2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4c0046aa688844fab54e5cde4008d15f","IPY_MODEL_659877a043314a7284a942ca18115d50"]}},"c8f6b37ce3e64f58a66cfc15a6ce39f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4c0046aa688844fab54e5cde4008d15f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ba2e8c464c46498a9b0e2682091e345a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aa8417546fb540e29e7102ee402b79b1"}},"659877a043314a7284a942ca18115d50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b39dae21b89345bfacea5dd000d105fe","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 8192/? [00:00&lt;00:00, 9963.83it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_40627eee9c4741c48850130664ecc625"}},"ba2e8c464c46498a9b0e2682091e345a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"aa8417546fb540e29e7102ee402b79b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b39dae21b89345bfacea5dd000d105fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"40627eee9c4741c48850130664ecc625":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"x1pdd3T87D7K"},"source":["# Generative Adversarial Network\r\n","\r\n","In this notebook, we'll be building a generative adversarial network (GAN) trained on the MNIST dataset. From this, we'll be able to generate new handwritten digits!\r\n","\r\n","GANs were [first reported on](https://arxiv.org/abs/1406.2661) in 2014 from Ian Goodfellow and others in Yoshua Bengio's lab. Since then, GANs have exploded in popularity. Here are a few examples to check out:\r\n","\r\n","* [Pix2Pix](https://affinelayer.com/pixsrv/)\r\n","* [CycleGAN & Pix2Pix in PyTorch, Jun-Yan Zhu](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\r\n","* [A list of generative models](https://github.com/wiseodd/generative-models)\r\n","\r\n","The idea behind GANs is that you have two networks, a generator $G$ and a discriminator $D$, competing against each other. The generator makes \"fake\" data to pass to the discriminator. The discriminator also sees real training data and predicts if the data its received is real or fake.\r\n","\r\n",">* The generator is trained to fool the discriminator, it wants to output data that looks as *close as possible* to the real, training data. \r\n",">* The discriminator is a classifier that is trained to figure out which data is real and which is fake.\r\n","\r\n","What ends up happening is that the generator learns to make data that is indistinguishable from real data to the discriminator.\r\n","\r\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/raw/3a95d118f9df5a86826e1791c5c100817f0fd924/gan-mnist/assets/gan_pipeline.png\" width=500>\r\n","\r\n","The general structure of a GAN is shown in the diagram above, using MNIST images as data. The latent sample is a random vector that the generator uses to construct its fake images. This is often called a **latent vector** and that vector space is called a **latent space**. As the generator trains, it figures out how to map latent vectors to recognizable images that can fool the discriminator.\r\n","\r\n","If you're interested in generating only new images, you can throw out the discriminator after training. In this notebook, we will define and train these adversarial networks in PyTorch and generate new images!"]},{"cell_type":"code","metadata":{"id":"J0VwvR0c99dg","executionInfo":{"status":"ok","timestamp":1612561526377,"user_tz":-60,"elapsed":3771,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["%matplotlib inline\r\n","\r\n","import numpy as np\r\n","import torch\r\n","import matplotlib.pyplot as plt\r\n","\r\n","from torchvision import datasets\r\n","import torchvision.transforms as transforms"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386,"referenced_widgets":["2a26dde2ea1d47eab49d7fc5ff27fef1","ef8f76ccba41452296b77e9d83253afb","50486b253e09492a958ffca7263170db","76c865cad5aa4509963501dd84643158","dbf3c5b4c8c74704948f48628d5c46a0","c9b5bc5865644115a5cc6bb41adb2862","b34dd037153e4740831dec7f89fe8647","4b804211c5c34f0090ca5163a715c006","4b95899b999e48229afd0a5ab8e9d8f2","fd349784bc3f4e8280c9d52b7cf3a0a1","42ab7588bb9341888497c2e7ff704f97","6ef0359d3f904df5b66d4cfe76393d18","0550694132784348a12bc0bc5abcbb4d","f7749db8f1a645bfbdde8baf8a46dd4a","f33ea0f49abf4e5c89d3d2be2b9ed14f","5979c18084d5432095fb957398a5b4a4","dfcee6a6c834462c9a7fde243f9c1925","020672a5870b4aacb7100ded9aea84ca","a130f54bf89244a78f2535ad52cba3d0","7a736623a40d4534a43f5bb4fcae072f","db5dec59fe10404d9d255c8414aba6fe","f4821c2fa7374e3983565f4293eba544","986c21b5e2c64b7792cee4b317e46d4c","49637f35116e4d91af5b9f5a47a7ac46","e64a8412e5f14f9bb79cb1705c3e7bc0","c8f6b37ce3e64f58a66cfc15a6ce39f2","4c0046aa688844fab54e5cde4008d15f","659877a043314a7284a942ca18115d50","ba2e8c464c46498a9b0e2682091e345a","aa8417546fb540e29e7102ee402b79b1","b39dae21b89345bfacea5dd000d105fe","40627eee9c4741c48850130664ecc625"]},"id":"ydlAJqvS-Gze","executionInfo":{"status":"ok","timestamp":1612561533519,"user_tz":-60,"elapsed":10893,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"afd4dd66-c753-42c8-df9d-482601eb8e1a"},"source":["# number of subprocesses to use for data loading\r\n","num_workers = 0\r\n","# how many samples per batch to load\r\n","batch_size = 64\r\n","\r\n","# convert data to torch.FloatTensor\r\n","transform = transforms.ToTensor()\r\n","\r\n","# get the training datasets\r\n","train_data = datasets.MNIST(root='data', train=True, download=True, \r\n","                            transform=transform)\r\n","\r\n","# prepare data loader\r\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\r\n","                                           num_workers=num_workers)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a26dde2ea1d47eab49d7fc5ff27fef1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b95899b999e48229afd0a5ab8e9d8f2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dfcee6a6c834462c9a7fde243f9c1925","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e64a8412e5f14f9bb79cb1705c3e7bc0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n","Processing...\n","Done!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"oURx2lN8-xUU"},"source":["### Visualize the data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"rdMM58_K-4nX","executionInfo":{"status":"ok","timestamp":1612561533872,"user_tz":-60,"elapsed":11237,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"8c60b725-7239-45bf-ff7a-7fb1fe89445c"},"source":["# obtain one batch of training images\r\n","dataiter = iter(train_loader)\r\n","images, labels = dataiter.next()\r\n","images = images.numpy()\r\n","\r\n","# Get one image from the batch\r\n","img = np.squeeze(images[0])\r\n","\r\n","fig = plt.figure(figsize=(3,3))\r\n","ax = fig.add_subplot(111)\r\n","ax.imshow(img, cmap='gray')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f7bc1033a20>"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 216x216 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WeJ7xcI6_Ifa"},"source":["---\r\n","## Define the Model\r\n","\r\n","A GAN is comprised of two adversarial networks, a discriminator and a generator.\r\n","\r\n","### Discriminator\r\n","\r\n","The discriminator network is going to be a pretty typical linear classifier. To make this network a universal approximator, we'll need at least one hidden layer, and these hidden layers should have one key attribute:\r\n","\r\n","> All hidden layers will have a [Leaky ReLU](https://pytorch.org/docs/stable/nn.html#torch.nn.LeakyReLU) activation function applied to their outputs.\r\n","\r\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/raw/3a95d118f9df5a86826e1791c5c100817f0fd924/gan-mnist/assets/gan_network.png\" width=300>\r\n","\r\n","#### Leaky ReLU\r\n","\r\n","We should use a leaky ReLU to allow gradients to flow backwards through the layer unimpeded. A leaky ReLU is like a normal ReLU, except that there is a small non-zero output for negative input values.\r\n","\r\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/raw/3a95d118f9df5a86826e1791c5c100817f0fd924/gan-mnist/assets/leaky_relu.png\" width=300>\r\n","\r\n","#### Sigmoid Output\r\n","\r\n","We'll also take the approach of using a more numerically stable loss function on the outputs. Recall that we want the discriminator to output a value 0-1 indicating whether an image is *real* or *fake*.\r\n","\r\n","> We will ultimately use [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss), which combines a `sigmoid` activation function **and** binary cross entropy loss in one function.\r\n","\r\n","So, our final output layer should not have any activation function applied to it."]},{"cell_type":"code","metadata":{"id":"_1MiRhgMA0ys","executionInfo":{"status":"ok","timestamp":1612561533873,"user_tz":-60,"elapsed":11232,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["import torch.nn as nn\r\n","import torch.nn.functional as F"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOG3HGVsA5eL","executionInfo":{"status":"ok","timestamp":1612561533874,"user_tz":-60,"elapsed":11229,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["class Discriminator(nn.Module):\r\n","\r\n","    def __init__(self, input_size, hidden_dim, output_size):\r\n","        super(Discriminator, self).__init__()\r\n","\r\n","        # define hidden layers\r\n","        self.fc1 = nn.Linear(input_size, hidden_dim*4)\r\n","        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\r\n","        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\r\n","\r\n","        # final, fully-connected layer\r\n","        self.fc4 = nn.Linear(hidden_dim, output_size)\r\n","\r\n","        # dropout layer\r\n","        self.dropout = nn.Dropout(0.3)\r\n","\r\n","    def forward(self, x):\r\n","        # flatten image\r\n","        x = x.view(-1, 28*28)\r\n","        # pass x through all layers\r\n","        # apply leaky ReLU ativation to all hidden layers\r\n","        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\r\n","        x = self.dropout(x)\r\n","        x = F.leaky_relu(self.fc2(x), 0.2)\r\n","        x = self.dropout(x)\r\n","        x = F.leaky_relu(self.fc3(x), 0.2)\r\n","        x = self.dropout(x)\r\n","        x = self.fc4(x)\r\n","        return x"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0XbpyyYaIpzI"},"source":["### Generator\r\n","\r\n","The generator network will be almost exactly the same as the discriminator network, except that we're applying a [tanh activation function](https://pytorch.org/docs/stable/nn.html#tanh) to our output layer.\r\n","\r\n","#### tanh Output\r\n","\r\n","The generator has been found to perform the best with $tanh$ for the generator output, which scales the output to be between -1 and 1, instead of 0 and 1.\r\n","\r\n","<img src='https://github.com/udacity/deep-learning-v2-pytorch/raw/3a95d118f9df5a86826e1791c5c100817f0fd924/gan-mnist/assets/tanh_fn.png'>\r\n","\r\n","Recall that we also want these outputs to be comparable to the *real* input pixels values, which are read in as normalized values between 0 and 1.\r\n","\r\n","> So, we'll also have to **scale our real input images to have pixel values between -1 and 1** when we train the discriminator.\r\n","\r\n","This will be done in the training loop, later on.\r\n"]},{"cell_type":"code","metadata":{"id":"HXouz4xQJ9NI","executionInfo":{"status":"ok","timestamp":1612562337334,"user_tz":-60,"elapsed":938,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["class Generator(nn.Module):\r\n","\r\n","    def __init__(self, input_size, hidden_dim, output_size):\r\n","\r\n","        super(Generator, self).__init__()\r\n","\r\n","        # define all layers\r\n","\r\n","        # Fully-connected layers\r\n","        self.fc1 = nn.Linear(input_size, hidden_dim)\r\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\r\n","        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\r\n","        self.fc4 = nn.Linear(hidden_dim*4, output_size)\r\n","\r\n","        # Hyperbolic Tangent\r\n","        self.tanh = nn.Tanh()\r\n","\r\n","        # dropout layer\r\n","        self.dropout = nn.Dropout(p=0.3)\r\n","\r\n","    def forward(self, x):\r\n","        # pass x through all layers\r\n","        x = F.leaky_relu(self.fc1(x), 0.2)\r\n","        x = self.dropout(x)\r\n","\r\n","        x = F.leaky_relu(self.fc2(x), 0.2)\r\n","        x = self.dropout(x)\r\n","\r\n","        x = F.leaky_relu(self.fc3(x), 0.2)\r\n","        x = self.dropout(x)\r\n","\r\n","        # final layer \r\n","        x = F.tanh(self.fc4(x))\r\n","\r\n","        return x"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y5ClKUzxL_co"},"source":["## Model hyperparameters"]},{"cell_type":"code","metadata":{"id":"gnpvRsOmNdKi","executionInfo":{"status":"ok","timestamp":1612562932188,"user_tz":-60,"elapsed":851,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["# Discriminator hyperparameters\r\n","\r\n","# Size of input image to discriminator (28*28)\r\n","input_size = 28*28\r\n","# Size of discriminator output (real or fake)\r\n","d_output_size = 1\r\n","# Size of *last* hidden layer in the discriminator\r\n","d_hidden_size = 32\r\n","\r\n","# Generator hyperparameters\r\n","\r\n","# Size of latent vector to give to generator\r\n","z_size = 100\r\n","# Size of generator output (generated image)\r\n","g_output_size = 28*28\r\n","# Size of *first* hidden layer in the generator\r\n","g_hidden_size = 32"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wQNwR88aOycD"},"source":["## Build complete network\r\n","\r\n","Now we're instantiating the discriminator and generator from the classes defined above. Make sure you've passed in the correct input arguments."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vU0zd4FVO7ht","executionInfo":{"status":"ok","timestamp":1612562934020,"user_tz":-60,"elapsed":841,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}},"outputId":"4128c6f6-4ae0-4bcd-faba-b66a230fa2bc"},"source":["# instantiate discriminator and generator\r\n","D = Discriminator(input_size, d_hidden_size, d_output_size)\r\n","G = Generator(z_size, g_hidden_size, g_output_size)\r\n","\r\n","# Check that they are as you expect\r\n","print(D)\r\n","print()\r\n","print(G)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Discriminator(\n","  (fc1): Linear(in_features=784, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n","  (dropout): Dropout(p=0.3, inplace=False)\n",")\n","\n","Generator(\n","  (fc1): Linear(in_features=100, out_features=32, bias=True)\n","  (fc2): Linear(in_features=32, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=128, bias=True)\n","  (fc4): Linear(in_features=128, out_features=784, bias=True)\n","  (tanh): Tanh()\n","  (dropout): Dropout(p=0.3, inplace=False)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cfQY01usPXBj"},"source":["---\r\n","## Discriminator and Generator Losses\r\n","\r\n","Now we need to calculate the losses.\r\n","\r\n","### Discriminator Losses\r\n",">* For the discriminator, the total loss is the sum of the losses for real and fake images, `d_loss = d_real_loss + d_fake_loss`.\r\n",">* Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to setup the losses to reflect that.\r\n","\r\n","<img src='https://github.com/udacity/deep-learning-v2-pytorch/raw/3a95d118f9df5a86826e1791c5c100817f0fd924/gan-mnist/assets/gan_pipeline.png'>\r\n","\r\n","The losses will be binary cross entropy loss with logits, which we can get with [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss). This combines a sigmoid activation function **and** binary cross entropy loss in one function.\r\n","\r\n","For the real images, we want `D(real_images)=1`. That is, we want the discriminator to classify the real images with a label=1, indicating that these are real. To help the discriminator generalize better, the labels are **reduced a bit from 1.0 to 0.9** For this, we'll use the parameter `smooth`; if True, then we should smooth our labels. In PyTorch, this looks like `labels=torch.ones(size) * 0.9`\r\n","\r\n","The discriminator loss for the fake data is similar. We want `D(fake_images)=0`, where the fake images are the *generator output*, `fake_images=G(z)`.\r\n","\r\n","### Generator Loss\r\n","\r\n","The generator loss will look similar only with flipped labels. The generator's goal is to get `D(fake_images) = 1`. In this case, the labels are **flipped** to represent that the generator is trying to fool the discriminator into thinking that the images it generates (fakes) are real!"]},{"cell_type":"code","metadata":{"id":"eAbFsX4pTCCP","executionInfo":{"status":"ok","timestamp":1612564972401,"user_tz":-60,"elapsed":764,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["# Calculate losses\r\n","\r\n","def real_loss(D_out, smooth=False):\r\n","    batch_size = D_out.size(0)\r\n","    # label smoothing\r\n","    if smooth:\r\n","        # smooth, real labels=0.9\r\n","        labels = torch.ones(batch_size)*0.9\r\n","    else:\r\n","        labels = torch.ones(batch_size) # real labels = 1\r\n","\r\n","    # numerically stable loss\r\n","    criterion = nn.BCEWithLogitsLoss()\r\n","    # calculate loss\r\n","    loss = criterion(D_out.squeeze(), labels)\r\n","    return loss\r\n","\r\n","def fake_loss(D_out):\r\n","    # compare logits to fake labels\r\n","    batch_size = D_out.size(0)\r\n","    labels = torch.zeros(batch_size) # fake labels = 0\r\n","    criterion = nn.BCEWithLogitsLoss()\r\n","    loss = criterion(D_out.squeeze(), labels)\r\n","    return loss"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZnFfDRMNUZna"},"source":["## Optimizers\r\n","\r\n","We want to update the generator and discriminator variables separately. So, we'll define two separate Adam optimizers."]},{"cell_type":"code","metadata":{"id":"snNTg6OWVnbq","executionInfo":{"status":"ok","timestamp":1612565081582,"user_tz":-60,"elapsed":807,"user":{"displayName":"Bob Bell","photoUrl":"","userId":"12813757050463534657"}}},"source":["import torch.optim as optim\r\n","\r\n","# learning rate for optimizers\r\n","lr = 0.002\r\n","\r\n","# Create optimizers for the discriminator and generator\r\n","d_optimizer = optim.Adam(D.parameters(), lr)\r\n","g_optimizer = optim.Adam(G.parameters(), lr)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RTdKMd6NXmnj"},"source":["---\r\n","## Training\r\n","\r\n","Training will involve alternating between training the discriminator and the generator. We'll use our functions `real_loss` and `fake_loss` to help us calculate the discriminator losses in all of the following cases.\r\n","\r\n","### Discriminator training\r\n","1. Compute the discriminator loss on real, training images\r\n","2. Generate fake images\r\n","3. Compute the discriminator loss on fake, generated images\r\n","4. Add up real and fake loss\r\n","5. Perform backpropagation + an optimization step to updte the discriminator's weights\r\n","\r\n","### Generator training\r\n","1. Generate fake images\r\n","2. Compute the discriminator loss on fake images, using **flipped** labels!\r\n","3. Perform backpropagation + an optimization step to update the generator's weights\r\n","\r\n","#### Saving Samples\r\n","\r\n","As we train, we'll also print out some loss statistics and save some generated \"fake\" samples."]},{"cell_type":"code","metadata":{"id":"Bg3_VBrxX-u2"},"source":["import pickle as pkl\r\n","\r\n","# training hyperparameters\r\n","num_epochs = 40\r\n","\r\n","# keep track of loss and generated, \"fake\" samples\r\n","samples = []\r\n","losses = []\r\n","\r\n","print_every = 400\r\n","\r\n","# Get some fixed data for sampling. These are images that are held\r\n","# constant throughout training, and allow us to inspect the model's performance\r\n","sample_size = 16\r\n","fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\r\n","fixed_z = torch.from_numpy(fixed_z).float()\r\n","\r\n","# train the network\r\n","D.train()\r\n","G.train()\r\n","\r\n","for epoch in range(num_epochs):\r\n","\r\n","    for batch_i, (real_images, _) in enumerate(train_loader):\r\n","        "],"execution_count":null,"outputs":[]}]}